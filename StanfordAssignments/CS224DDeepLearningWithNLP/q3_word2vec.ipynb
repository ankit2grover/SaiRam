{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "dimensions = [10,5, 10]\n",
    "data = np.random.randn(N, dimensions[0])\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i, random.randint(0, dimensions[2] -1)] = 1\n",
    "    \n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    sig_f = 1 / (1 + np.exp( - x))\n",
    "    return sig_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(f):\n",
    "    g = f * (1- f)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 1:\n",
    "        x -= np.min(x)\n",
    "        x = np.exp(x)\n",
    "        x /= np.sum(x)\n",
    "    else:\n",
    "        x -= np.min(x, axis = 1, keepdims = True)\n",
    "        x = np.exp(x)\n",
    "        x /= np.sum(x, axis = 1, keepdims = True)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        x[ix] += h \n",
    "        print x\n",
    "        random.setstate(rndstate)\n",
    "        fxph = f(x)[0]\n",
    "        print fxph\n",
    "        x[ix] -= 2 * h\n",
    "        random.setstate(rndstate)\n",
    "        fxmh = f(x)[0]\n",
    "        print fxmh\n",
    "        x[ix] += h\n",
    "        numgrad = (fxph - fxmh) / (2 * h)  \n",
    "        print numgrad\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t: t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t: t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t: t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t: t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    h = sigmoid(data.dot(W1) + b1)\n",
    "    scores = softmax(h.dot(W2) + b2)\n",
    "    cost = np.sum(-np.log(scores[labels ==1])) / N\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ## Compare softmax output with original output\n",
    "    #print scores[1]\n",
    "    #print labels[1]\n",
    "    \n",
    "    dscores = scores - labels\n",
    "    #print dscores[1]\n",
    "    dscores = dscores / N\n",
    "    #print dscores[1]\n",
    "    gradb2 = np.sum(dscores, axis=0)\n",
    "    gradW2 = np.dot(h.T, dscores)\n",
    "    \n",
    "    grad_h = np.dot(dscores, W2.T)\n",
    "    grad_h = sigmoid_grad(h) * grad_h\n",
    "    \n",
    "    gradb1 = np.sum(grad_h, axis=0)\n",
    "    gradW1 = np.dot(data.T, grad_h)\n",
    "    g1 = gradW1.flatten()\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((g1, gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    print grad.shape\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forward_backward_prop(data, labels, params)\n",
    "# Perform gradcheck on your neural network\n",
    "print \"=== For autograder ===\"\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "def normalizeRows(x):\n",
    "    \"\"\"Row normalization function\"\"\"\n",
    "    N = x.shape[0]\n",
    "    x /= np.sqrt(np.sum(x ** 2, axis = 1)).reshape(N, 1)\n",
    "    return x\n",
    "\n",
    "print normalizeRows(np.array([[3.0,4.0],[1, 2]]))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement your skip-gram and CBOW models here\n",
    "\n",
    "# Interface to the dataset for negative sampling\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, assuming the softmax prediction function and cross      #\n",
    "    # entropy loss.                                                   #\n",
    "    # Inputs:                                                         #\n",
    "    #   - predicted: numpy ndarray, predicted word vector (\\hat{r} in #\n",
    "    #           the written component)                                #\n",
    "    #   - target: integer, the index of the target word               #\n",
    "    #   - outputVectors: \"output\" vectors for all tokens              #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: cross entropy cost for the softmax word prediction    #\n",
    "    #   - gradPred: the gradient with respect to the predicted word   #\n",
    "    #           vector                                                #\n",
    "    #   - grad: the gradient with respect to all the other word       # \n",
    "    #           vectors                                               #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the skip-gram model in this function.                 #         \n",
    "    # Inputs:                                                         #\n",
    "    #   - currrentWord: a string of the current center word           #\n",
    "    #   - C: integer, context size                                    #\n",
    "    #   - contextWords: list of no more than 2*C strings, the context #\n",
    "    #             words                                               #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - inputVectors: \"input\" word vectors for all tokens           #\n",
    "    #   - outputVectors: \"output\" word vectors for all tokens         #\n",
    "    #   - word2vecCostAndGradient: the cost and gradient function for #\n",
    "    #             a prediction vector given the target word vectors,  #\n",
    "    #             could be one of the two cost functions you          #\n",
    "    #             implemented above                                   #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: the cost function value for the skip-gram model       #\n",
    "    #   - grad: the gradient with respect to the word vectors         #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2, :]\n",
    "    outputVectors = wordVectors[N/2:, :]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1, C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, word2vecCostAndGradient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
